{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color: orange'>Équipe: Louis & Emire</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparaison des performances entre les méthodes linéaires vs méthodes non-linéaires pour la prédiction sur de vraies données transcriptomiques des cancers de TCGA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workplan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Review litterature about the type of ML algorithms and their models\n",
    "2. Data importation: `TCGA dataset`\n",
    "3. Data preprocessing -> experience data\n",
    "4. Algorithms/models configuration\n",
    "5. Models training\n",
    "6. Comparison with figures\n",
    "   - 2 clusters containing maximum 4 figures each.\n",
    "   - 1 classification plot for each algorithm (2 in total for each cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from utils.random_color_generator import random_color_generator\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DataSet import DataSet\n",
    "\n",
    "# Data importation and pre-processing\n",
    "\n",
    "data_set = DataSet('./data/TCGA_TPM_hv_subset.h5')\n",
    "\n",
    "figures_directory = \"./figures/project\"\n",
    "\n",
    "data = data_set.get_data('data', float)\n",
    "\n",
    "genes = data_set.get_data('cols', str)\n",
    "labels = data_set.get_data('labels', str)\n",
    "rows = data_set.get_data('rows', str)\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pour la classification moléculaires entre la régression linéaire et un DNN, quelle méthode fonctionne le mieux et pourquoi?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.T, labels, test_size= 0.2)\n",
    "log_clf = RidgeClassifier(alpha=1e-4, solver =\"lsqr\", max_iter = max_iter)\n",
    "log_clf.fit(X_train, Y_train)\n",
    "\n",
    "train_set_accuracy = round((np.mean(log_clf.predict(X_train) == Y_train)), 2) * 100\n",
    "test_set_accuracy = round((np.mean(log_clf.predict(X_test) == Y_test)), 2) * 100\n",
    "\n",
    "print(f\"Accuracy on train set (n = {X_train.shape[0]}): {train_set_accuracy}%\")\n",
    "print(f\"Accuracy on test set (n = {X_test.shape[0]}): {test_set_accuracy}%\")\n",
    "print(f\"Number of errors : {(np.sum(log_clf.predict(X_test) != Y_test)) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(log_clf.predict(X_test),  Y_test)\n",
    "\n",
    "# fig = plt.figure(figsize = (10, 10))\n",
    "axs[0,0].imshow(cm, cmap='Oranges', vmin=cm.min(), vmax=cm.max())\n",
    "axs[0,0].set_xticks(np.arange(len(np.unique(labels))), labels = np.unique(labels), rotation = 45, ha='right')\n",
    "axs[0,0].set_yticks(np.arange(len(np.unique(labels))), labels = np.unique(labels))\n",
    "axs[0,0].set_xlabel(\"Predicted Labels\")\n",
    "axs[0,0].set_ylabel(\"True Labels\")\n",
    "axs[0,0].set_title(\"Ridge classifier confusion matrix on test set\")\n",
    "\n",
    "thresh = cm.max() / 1.5\n",
    "\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    if cm[i, j] != 0:\n",
    "        axs[0,0].text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "        horizontalalignment=\"center\",\n",
    "    color=\"white\" if cm[i, j] > thresh else \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdff4ad90c0>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions = log_clf.predict(X_test)\n",
    "\n",
    "# Find indices where predictions and actual values differ\n",
    "error_indices = np.where(predictions != Y_test)[0]\n",
    "\n",
    "# plt.figure(figsize = (8, 8))\n",
    "# Assuming your data is 2D, plot the first two features for simplicity\n",
    "# Adjust this based on your actual data\n",
    "axs[1,0].scatter(X_test[error_indices, 0], X_test[error_indices, 1], color='red', label='Errors')\n",
    "\n",
    "# Optionally, plot correct predictions for comparison\n",
    "correct_indices = np.where(predictions == Y_test)[0]\n",
    "axs[1,0].scatter(X_test[correct_indices, 0], X_test[correct_indices, 1], color='green', alpha=0.5, label='Correct')\n",
    "\n",
    "axs[1,0].set_xlabel('RC1')\n",
    "axs[1,0].set_ylabel('RC2')\n",
    "axs[1,0].set_title(f'Ridge classifier errors plot on the test set\\nAccuracy : {test_set_accuracy}%, Iters = {max_iter}')\n",
    "axs[1,0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lblbin = LabelBinarizer()\n",
    "\n",
    "targets = lblbin.fit_transform(labels)\n",
    "\n",
    "X, Y = torch.Tensor(data.T), torch.Tensor(targets)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = [torch.Tensor(split) for split in train_test_split(data.T, targets, test_size = 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(insize, 100), # couche entrée\n",
    "            nn.Linear(100, 100), # couche cahée\n",
    "            nn.Linear(100, outsize)  # couche sortie\n",
    "        )\n",
    "\n",
    "    def train(self, nepochs = 1000, printstep = 100):\n",
    "        mm = self.linear_relu_stack        \n",
    "        optimizer = torch.optim.Adam(mm.parameters(), lr = 1e-4)\n",
    "        tr_losses, tst_losses, tr_accs, tst_accs = [], [], [], [] \n",
    "        # boucle de training \n",
    "        for i in range(nepochs):\n",
    "            #####\n",
    "            optimizer.zero_grad() # required*\n",
    "            out_tr = mm(X_train) # calcul 'avant'\n",
    "            tr_error = nn.functional.cross_entropy(out_tr, Y_train)\n",
    "            out_test = mm(X_test)\n",
    "            tst_error = nn.functional.cross_entropy(out_test, Y_test)\n",
    "            tr_acc = np.mean(np.array( out_tr.max(1).indices == Y_train.max(1).indices))\n",
    "            tst_acc = np.mean(np.array( out_test.max(1).indices == Y_test.max(1).indices))\n",
    "            [tr_losses.append(float(tr_error)),tst_losses.append(float(tst_error)),\n",
    "            tr_accs.append(tr_acc), tst_accs.append(tst_acc) ]\n",
    "            tr_error.backward() # calcul 'arriere' et mise a jour\n",
    "            optimizer.step() # required*\n",
    "            if i % printstep == 0 or i == nepochs - 1:\n",
    "                print(f\"{i} Loss Train : {round(float(tr_error) * 100,2)} - Acc: {round(tr_acc * 100,2)} \\\n",
    "                    - Test : {round(float(tst_error) * 100,2)} - Acc: {round(tst_acc * 100,2)} \\\n",
    "                        - errors: {np.sum(np.array( out_test.max(1).indices != Y_test.max(1).indices))}\")\n",
    "        return mm, tr_losses, tst_losses, np.array(tr_accs), np.array(tst_accs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nni = NeuralNetwork(X_train.shape[1], Y_train.shape[1])\n",
    "mm, tr_losses, tst_losses, tr_accs, tst_accs = nni.train(nepochs = 100, printstep = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Learning curves of DNN on classification of cancer type in TCGA data\\nN=15165, N(train)=8276, N(test)=2070')"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axs[1, 1].cla()\n",
    "\n",
    "markers_ = np.concatenate([['o',\"v\",\"^\",\"<\",\">\",\"8\",\"p\",\"s\",\"h\",\"D\",\"P\",\"X\"] for i in range(10)])\n",
    "\n",
    "steps = np.arange(len(tr_losses))\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [Line2D([0], [0], color='green', lw=2),\n",
    "                Line2D([0], [0], color='blue', lw=2),\n",
    "                Line2D([0], [0], color='orange', lw=2, linestyle='--'),\n",
    "                Line2D([0], [0], color='purple', lw=2, linestyle='--')]\n",
    "\n",
    "# First plot (Losses) on axs[0, 1]\n",
    "axs[1, 1].plot(steps, tr_losses, label=\"train (loss)\", color='green')\n",
    "axs[1, 1].plot(steps, tst_losses, label=\"test (loss)\", color='blue')\n",
    "axs[1, 1].plot(steps[-1], tr_losses[-1], 'go', markersize=5, label=f'({steps[-1]}, {tr_losses[-1]:.2f})')\n",
    "axs[1, 1].plot(steps[-1], tst_losses[-1], 'bv', markersize=5, label=f'({steps[-1]}, {tst_losses[-1]:.2f})')\n",
    "axs[1, 1].set_ylabel(\"Cross entropy loss\")\n",
    "axs[1, 1].set_xlabel(\"Epochs number\")\n",
    "axs[1, 1].legend(loc='center right', bbox_to_anchor=(1, 0.575))\n",
    "\n",
    "# Second plot (Accuracies) on the same subplot, but using a secondary y-axis\n",
    "sec_ax = axs[1, 1].twinx()\n",
    "sec_ax.plot(steps, tr_accs * 100, label=\"train (accuracy)\", color='orange')\n",
    "sec_ax.plot(steps, tst_accs * 100, label=\"test (accuracy)\", color='purple')\n",
    "sec_ax.plot(steps[-1], tr_accs[-1] * 100, 'o', color='orange', markersize=5, label=f'({steps[-1]}, {tr_accs[-1]*100:.2f}%)')\n",
    "sec_ax.plot(steps[-1], tst_accs[-1] * 100, 'v', color='purple', markersize=5, label=f'({steps[-1]}, {tst_accs[-1]*100:.2f}%)')\n",
    "sec_ax.set_ylabel(\"Accuracy (%)\")\n",
    "sec_ax.set_ylim((0, 100))\n",
    "sec_ax.legend(loc='center right', bbox_to_anchor=(1, 0.425))\n",
    "\n",
    "# Set the title for the combined plot\n",
    "axs[1, 1].set_title(f\"Learning curves of DNN on classification of cancer type in TCGA data\\nN={data.T.shape[1]}, N(train)={X_train.shape[0]}, N(test)={X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(log_clf.predict(X_test),  Y_test)\n",
    "\n",
    "axs[0,1].imshow(cm, cmap='Blues', vmin=cm.min(), vmax=cm.max())\n",
    "axs[0,1].set_xticks(np.arange(len(np.unique(labels))), labels = np.unique(labels), rotation = 45, ha='right')\n",
    "axs[0,1].set_yticks(np.arange(len(np.unique(labels))), labels = np.unique(labels))\n",
    "axs[0,1].set_xlabel(\"Predicted Labels\")\n",
    "axs[0,1].set_ylabel(\"True Labels\")\n",
    "axs[0,1].set_title(\"DNN confusion matrix on test set\")\n",
    "\n",
    "thresh = cm.max() / 1.5\n",
    "\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    if cm[i, j] != 0:\n",
    "        axs[0,1].text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "        horizontalalignment=\"center\",\n",
    "    color=\"white\" if cm[i, j] > thresh else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f'{figures_directory}/project_question_a_grouped_figures.svg', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Pour la régression entre la régression linéaire et un Auto-Encodeur, quelle méthode fonctionne le mieux et pourquoi?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Démontrez qu'une PCA 2D est équivalent à la couche interne bottleneck d'Auto-Encodeur linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
